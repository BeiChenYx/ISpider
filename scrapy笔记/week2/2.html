<!DOCTYPE HTML>
<html lang="en" >
    
    <head>
        
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <title>2. 淘宝商品爬虫项目实战 | Introduction</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="generator" content="GitBook 2.6.7">
        
        
        <meta name="HandheldFriendly" content="true"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black">
        <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
        <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">
        
    <link rel="stylesheet" href="../gitbook/style.css">
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-highlight/website.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-search/search.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-fontsettings/website.css">
        
    
    

        
    
    
    <link rel="next" href="../week2/3.html" />
    
    
    <link rel="prev" href="../week2/1.html" />
    

        
    </head>
    <body>
        
        
    <div class="book"
        data-level="12.2"
        data-chapter-title="2. 淘宝商品爬虫项目实战"
        data-filepath="week2/2.md"
        data-basepath=".."
        data-revision="Sun Jan 28 2018 19:48:10 GMT+0800 (中国标准时间)"
        data-innerlanguage="">
    

<div class="book-summary">
    <nav role="navigation">
        <ul class="summary">
            
            
            
            

            

            
    
        <li class="chapter " data-level="0" data-path="index.html">
            
                
                    <a href="../index.html">
                
                        <i class="fa fa-check"></i>
                        
                        Introduction
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1" data-path="index.html">
            
                
                    <a href="../index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.</b>
                        
                        Scrapy简介
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2" data-path="scrapy/index.html">
            
                
                    <a href="../scrapy/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.</b>
                        
                        1. 认识Scrapy框架
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3" data-path="scrapy/2.html">
            
                
                    <a href="../scrapy/2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.</b>
                        
                        2. Scrapy安装和使用
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4" data-path="scrapy/3.html">
            
                
                    <a href="../scrapy/3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.</b>
                        
                        3. Scrapy常用命令实战
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="5" data-path="scrapy/4.html">
            
                
                    <a href="../scrapy/4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>5.</b>
                        
                        4. Scrapy实现当当网商品爬虫实战
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="6" data-path="scrapy/5.html">
            
                
                    <a href="../scrapy/5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>6.</b>
                        
                        5. Scrapy模拟登录实战
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="7" data-path="scrapy/6.html">
            
                
                    <a href="../scrapy/6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>7.</b>
                        
                        6. Scrapy新闻爬虫项目实战上
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="8" data-path="scrapy/7.html">
            
                
                    <a href="../scrapy/7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>8.</b>
                        
                        7. Scrapy新闻爬虫项目实战下
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="9" data-path="scrapy/8.html">
            
                
                    <a href="../scrapy/8.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>9.</b>
                        
                        8. Scrapy豆瓣网登录爬虫与验证码识别项目实战1
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="10" data-path="scrapy/9.html">
            
                
                    <a href="../scrapy/9.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>10.</b>
                        
                        9. Scrapy豆瓣网登录爬虫与验证码识别项目实战2
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="11" data-path="scrapy/10.html">
            
                
                    <a href="../scrapy/10.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>11.</b>
                        
                        10. 如何在Urillib中使用Xpath表达式
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="12" data-path="week2/index.html">
            
                
                    <a href="../week2/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>12.</b>
                        
                        Scrapy第二周实战
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="12.1" data-path="week2/1.html">
            
                
                    <a href="../week2/1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>12.1.</b>
                        
                        1. Scrapy与Urllib整合实现京东商品
                    </a>
            
            
        </li>
    
        <li class="chapter active" data-level="12.2" data-path="week2/2.html">
            
                
                    <a href="../week2/2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>12.2.</b>
                        
                        2. 淘宝商品爬虫项目实战
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="12.3" data-path="week2/3.html">
            
                
                    <a href="../week2/3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>12.3.</b>
                        
                        3. BeautifulSoup基础实战
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="12.4" data-path="week2/4.html">
            
                
                    <a href="../week2/4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>12.4.</b>
                        
                        4. PhantomJS基础实战
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="12.5" data-path="week2/5.html">
            
                
                    <a href="../week2/5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>12.5.</b>
                        
                        5. 腾讯动漫项目爬虫实战
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="12.6" data-path="week2/6.html">
            
                
                    <a href="../week2/6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>12.6.</b>
                        
                        6. Docker基础
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="12.7" data-path="week2/7.html">
            
                
                    <a href="../week2/7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>12.7.</b>
                        
                        7. Redis的使用
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="12.8" data-path="week2/8.html">
            
                
                    <a href="../week2/8.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>12.8.</b>
                        
                        8. 分布式爬虫的实战
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    


            
            <li class="divider"></li>
            <li>
                <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
                    Published with GitBook
                </a>
            </li>
            
        </ul>
    </nav>
</div>

    <div class="book-body">
        <div class="body-inner">
            <div class="book-header" role="navigation">
    <!-- Actions Left -->
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="../" >Introduction</a>
    </h1>
</div>

            <div class="page-wrapper" tabindex="-1" role="main">
                <div class="page-inner">
                
                
                    <section class="normal" id="section-">
                    
                        <h1 id="2-&#x6DD8;&#x5B9D;&#x5546;&#x54C1;&#x5927;&#x578B;&#x722C;&#x866B;&#x9879;&#x76EE;&#x4E0E;&#x81EA;&#x52A8;&#x5199;&#x5165;&#x6570;&#x636E;&#x5E93;&#x5B9E;&#x6218;">2. &#x6DD8;&#x5B9D;&#x5546;&#x54C1;&#x5927;&#x578B;&#x722C;&#x866B;&#x9879;&#x76EE;&#x4E0E;&#x81EA;&#x52A8;&#x5199;&#x5165;&#x6570;&#x636E;&#x5E93;&#x5B9E;&#x6218;</h1>
<h3 id="&#x5B9E;&#x6218;&#x5206;&#x6790;">&#x5B9E;&#x6218;&#x5206;&#x6790;</h3>
<ul>
<li>&#x5728;&#x6DD8;&#x5B9D;&#x5546;&#x54C1;&#x7F51;&#x7AD9;&#x4E2D;&#x8F93;&#x5165;&#x5173;&#x952E;&#x5B57;&#xFF0C;&#x641C;&#x7D22;&#x5546;&#x54C1;&#x4FE1;&#x606F;&#x7684;&#x722C;&#x53D6;</li>
</ul>
<pre><code>* https://s.taobao.com/search?ie=utf8&amp;initiative_id=staobaoz_20180125&amp;stats_click=search_radio_all%3A1&amp;js=1&amp;imgfile=&amp;q=%E5%9D%9A%E6%9E%9C&amp;suggest=0_1&amp;_input_charset=utf-8&amp;wq=jianguo&amp;suggest_query=jianguo&amp;source=suggest
* https://s.taobao.com/search?ie=utf8&amp;initiative_id=staobaoz_20180125&amp;stats_click=search_radio_all%3A1&amp;js=1&amp;imgfile=&amp;q=%E5%9D%9A%E6%9E%9C&amp;suggest=0_1&amp;_input_charset=utf-8&amp;wq=jianguo&amp;suggest_query=jianguo&amp;source=suggest&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=1%2C48&amp;s=44
* https://s.taobao.com/search?q=&#x575A;&#x679C;&amp;_input_charset=utf-8&amp;s=44
</code></pre><h4 id="1-&#x521B;&#x5EFA;scrapy&#x7684;&#x9879;&#x76EE;&#x548C;&#x722C;&#x866B;&#x6587;&#x4EF6;">1. &#x521B;&#x5EFA;Scrapy&#x7684;&#x9879;&#x76EE;&#x548C;&#x722C;&#x866B;&#x6587;&#x4EF6;</h4>
<pre><code>   $ scrapy startproject shop
        New Scrapy project &apos;shop&apos;, using template directory &apos;...&apos;, created in:
        C:\Users\Administrator\Desktop\code\shop

        You can start your first spider with:
            cd shop
            scrapy genspider example example.com

   $ cd shop

   $ scrapy genspider -t basic tb taobao.com
        Created spider &apos;tb&apos; using template &apos;basic&apos; in module:
        shop.spiders.tb
</code></pre><h4 id="2-&#x7F16;&#x5199;items&#x6587;&#x4EF6;">2. &#x7F16;&#x5199;items&#x6587;&#x4EF6;</h4>
<pre><code class="lang-python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>

<span class="hljs-comment"># Define here the models for your scraped items</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># See documentation in:</span>
<span class="hljs-comment"># https://doc.scrapy.org/en/latest/topics/items.html</span>

<span class="hljs-keyword">import</span> scrapy

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ShopItem</span><span class="hljs-params">(scrapy.Item)</span>:</span>
    <span class="hljs-comment"># define the fields for your item here like:</span>
    <span class="hljs-comment"># name = scrapy.Field()</span>
    <span class="hljs-comment">#&#x5546;&#x54C1;&#x6807;&#x9898;</span>
    title = scrapy.Field()
    <span class="hljs-comment">#&#x5546;&#x54C1;&#x94FE;&#x63A5;</span>
    link = scrapy.Field()
    <span class="hljs-comment">#&#x5546;&#x54C1;&#x4EF7;&#x683C;</span>
    price= scrapy.Field()
    <span class="hljs-comment">#&#x5546;&#x54C1;&#x8BC4;&#x8BBA;&#x6570;</span>
    comment = scrapy.Field()
</code></pre>
<h4 id="3-&#x7F16;&#x8F91;&#x914D;&#x7F6E;&#x6587;&#x4EF6;&#xFF1A;settingspy">3. &#x7F16;&#x8F91;&#x914D;&#x7F6E;&#x6587;&#x4EF6;&#xFF1A;settings.py</h4>
<pre><code class="lang-python">...
<span class="hljs-comment">#&#x8BBE;&#x7F6E;&#x5FFD;&#x7565;robots</span>
<span class="hljs-comment"># Obey robots.txt rules</span>
ROBOTSTXT_OBEY = <span class="hljs-keyword">False</span>
...

<span class="hljs-comment">#&#x5F00;&#x542F;item Pipeline&#x7684;&#x6570;&#x636E;&#x5904;&#x7406;</span>
<span class="hljs-comment"># Configure item pipelines</span>
ITEM_PIPELINES = {
    <span class="hljs-string">&apos;shop.pipelines.ShopPipeline&apos;</span>: <span class="hljs-number">300</span>,
}
...
</code></pre>
<h4 id="4-&#x7F16;&#x5199;tbpy&#x722C;&#x866B;&#x6587;&#x4EF6;&#xFF08;&#x7B2C;&#x4E00;&#x6B65;&#x5148;&#x722C;&#x53D6;&#x6307;&#x5B9A;&#x5173;&#x952E;&#x5B57;&#x4FE1;&#x606F;&#x7684;&#x5546;&#x54C1;id&#x53F7;&#xFF08;10&#x9875;&#x4E3A;&#x4F8B;&#xFF09;&#xFF09;">4. &#x7F16;&#x5199;tb.py&#x722C;&#x866B;&#x6587;&#x4EF6;&#xFF08;&#x7B2C;&#x4E00;&#x6B65;&#x5148;&#x722C;&#x53D6;&#x6307;&#x5B9A;&#x5173;&#x952E;&#x5B57;&#x4FE1;&#x606F;&#x7684;&#x5546;&#x54C1;ID&#x53F7;&#xFF08;10&#x9875;&#x4E3A;&#x4F8B;&#xFF09;&#xFF09;</h4>
<pre><code># -*- coding: utf-8 -*-
import scrapy
import urllib.request
import ssl
from scrapy.http import Request
import re
from shop.items import ShopItem

class TbSpider(scrapy.Spider):
    name = &apos;tb&apos;
    allowed_domains = [&apos;taobao.com&apos;]
    start_urls = [&apos;http://www.taobao.com/&apos;]

    def parse(self, response):
        key=&quot;&#x575A;&#x679C;&quot; #&#x641C;&#x7D22;&#x7684;&#x5173;&#x952E;&#x5B57;
        # &#x722C;&#x53D6;10&#x9875;
        for i in range(0,10):
            #&#x62FC;&#x88C5;url&#x5730;&#x5740;&#xFF0C;&#x5173;&#x952E;&#x5B57;&#x548C;&#x6BCF;&#x9875;&#x7684;&#x6570;&#x636E;&#x6761;&#x6570;&#x4FE1;&#x606F;
            url=&quot;https://s.taobao.com/search?q=&quot;+key+&quot;&amp;_input_charset=utf-8&amp;s=&quot;+str(i*44)
            yield Request(url=url,callback=self.page)

    def page(self,response):
        #&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5339;&#x914D;&#x51FA;&#x6240;&#x6709;&#x5546;&#x54C1;id&#x53F7;&#x4FE1;&#x606F;
        body = response.body.decode(&quot;utf-8&quot;,&quot;ignore&quot;)
        patid=&apos;&quot;nid&quot;:&quot;(.*?)&quot;&apos;
        allid=re.compile(patid).findall(body)
        print(allid)
</code></pre><ul>
<li>&#x6D4B;&#x8BD5;&#x7ED3;&#x679C;</li>
</ul>
<pre><code>C:\Users\Administrator\Desktop\code\shop&gt;scrapy crawl tb --nolog
[&apos;552854653561&apos;, &apos;542849553016&apos;, &apos;27482872016&apos;, &apos;536180042105&apos;, &apos;553863522887&apos;,
&apos;536914510847&apos;, &apos;561811425038&apos;, &apos;562929353939&apos;, &apos;534292461033&apos;, &apos;531965963519&apos;,
&apos;20531331862&apos;, &apos;525309360464&apos;, &apos;543358612807&apos;, &apos;38874627213&apos;, &apos;524795049267&apos;, &apos;5
53875602989&apos;, &apos;537176718257&apos;, &apos;520848078498&apos;, &apos;43097375502&apos;, &apos;40663494639&apos;, &apos;540
361270084&apos;, &apos;41045966568&apos;, &apos;36061059582&apos;, &apos;534603010322&apos;, &apos;539512909127&apos;, &apos;17077
692967&apos;, &apos;18072494936&apos;, &apos;540703003208&apos;, &apos;556923025304&apos;, &apos;540702311589&apos;, &apos;5359816
10055&apos;, &apos;563787282656&apos;, &apos;540702875061&apos;, &apos;560073792849&apos;, &apos;525489053889&apos;, &apos;5437861
33119&apos;]
[&apos;536805173184&apos;, &apos;45524511648&apos;, &apos;561527135379&apos;, &apos;538459445794&apos;, &apos;552854653561&apos;,
&apos;530587582641&apos;, &apos;546242488261&apos;, &apos;43126119309&apos;, &apos;539686923712&apos;, &apos;41183056908&apos;, &apos;5
50650042807&apos;, &apos;13996223906&apos;, &apos;539309718971&apos;, &apos;556833008091&apos;, &apos;563947927675&apos;, &apos;55
6295961861&apos;, &apos;537490839361&apos;, &apos;43575689371&apos;, &apos;35109052504&apos;, &apos;37985755179&apos;, &apos;56172
2406280&apos;, &apos;560278764719&apos;, &apos;554754996692&apos;, &apos;544499765848&apos;, &apos;45243249916&apos;, &apos;543209
852830&apos;, &apos;560407634657&apos;, &apos;557975502542&apos;, &apos;557975250286&apos;, &apos;559257476187&apos;, &apos;562662
589661&apos;, &apos;558632705481&apos;, &apos;560837790729&apos;, &apos;527028460302&apos;, &apos;537278078123&apos;, &apos;535758
700982&apos;, &apos;540764040000&apos;, &apos;562132074613&apos;, &apos;555696156032&apos;, &apos;547082576918&apos;, &apos;543498
198181&apos;, &apos;557995506976&apos;, &apos;547455799923&apos;, &apos;544942589965&apos;]
</code></pre><h4 id="5-&#x7F16;&#x8F91;tbpy&#x722C;&#x866B;&#x6587;&#x4EF6;&#xFF08;&#x7B2C;&#x4E8C;&#x6B65;&#x722C;&#x53D6;&#x6BCF;&#x4E2A;&#x680F;&#x76EE;&#x4FE1;&#x606F;&#x4E2D;&#x5217;&#x8868;&#x53F7;&#xFF09;">5. &#x7F16;&#x8F91;tb.py&#x722C;&#x866B;&#x6587;&#x4EF6;&#xFF08;&#x7B2C;&#x4E8C;&#x6B65;&#x722C;&#x53D6;&#x6BCF;&#x4E2A;&#x680F;&#x76EE;&#x4FE1;&#x606F;&#x4E2D;&#x5217;&#x8868;&#x53F7;&#xFF09;</h4>

                    
                    </section>
                
                
                </div>
            </div>
        </div>

        
        <a href="../week2/1.html" class="navigation navigation-prev " aria-label="Previous page: 1. Scrapy与Urllib整合实现京东商品"><i class="fa fa-angle-left"></i></a>
        
        
        <a href="../week2/3.html" class="navigation navigation-next " aria-label="Next page: 3. BeautifulSoup基础实战"><i class="fa fa-angle-right"></i></a>
        
    </div>
</div>

        
<script src="../gitbook/app.js"></script>

    
    <script src="../gitbook/plugins/gitbook-plugin-search/lunr.min.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-search/search.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-sharing/buttons.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-fontsettings/buttons.js"></script>
    

<script>
require(["gitbook"], function(gitbook) {
    var config = {"highlight":{},"search":{"maxIndexSize":1000000},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2}};
    gitbook.start(config);
});
</script>

        
    </body>
    
</html>
