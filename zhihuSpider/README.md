# zhihuSpider项目说明
## 问题描述
> 基础要求：把知乎的所有话题、各话题内的文章、问答爬下来（至少爬10万条数据，不设上限），把数据存储到数据库中，手段不限（可以使用Scrapy也可以使用Urllib,具体技术不做限制）。

## 项目具体要求

> 1. 把知乎所有话题爬下来存入数据库中(数据库建议使用mysql);
> 2. 根据各话题信息，构造各话题的文章列表，把所有文章名、文章链接等信息爬到数据库中。（考虑到时间问题，可以不用爬完，至少总文章数据爬10万条，不设上限）
> 3. 把文章具体信息（如果是专栏文章需要爬包括文章标题、作者、文章内容，如果是问答内容需要爬包括问题标题，问题描述，答案内容，答案作者等信息）爬下来存储到数据库中，问答类信息与文章类信息存储到不同数据库中。
> 4. 有能力可以采用分布式爬虫架构。


## 项目说明
#### 整体思路：

> * 使用scrapy框架完成整体架构的设计.
> * 在开始请求中访问[登录页面](https://www.zhihu.com/signup?next=%2F).
> * 准备好验证码和用户名密码等信息后使用表单提交.
> * 登录成功后请求[初始话题页面](https://www.zhihu.com/topics),在初始话题页面提取所有话题的 *data_id* (此参数主要用来获取话题的url).
> * 根据提取来的 *data_id* 和时间戳来拼接出所要的地址，然后使用 *requests* 接口获取所有的话题信息，通过 *ZhihuspiderItem* 和 *ZhihuspiderPipeline* 来处理数据的存储，使用 *MysqlOperation* 封装 *pymysql* 库来完成数据的入库。
> * 然后每一个问题和文章单独区分，使用同步处理数据存储问题。
> * 2018-25完成了

#### 重构方案:

> 1. 使用requests库全部重构。
> 2. 不做登录功能，请求到[第一个页面](https://www.zhihu.com/topics)时，获取第一级话题所有类别，然后开启多线程和分布式任务(每个主机同时请求任务，主机上的多线程共同完成请求,即分布式加多线程方式)来获取每个一级类别的所有二级类别，从而生成任务列表。
> 3. 多个主机获取任务，交个主机的多线程去处理。
> 4. 数据库采用ORM方式处理存储效率问题。
